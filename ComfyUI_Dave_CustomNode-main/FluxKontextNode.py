# Made by Davemane42#0042 for ComfyUI - Flux Kontext Integration 2025
import torch
import logging
import numpy as np
from typing import Dict, List, Tuple, Optional, Any
import json
import os
from pathlib import Path

# å¯¼å…¥ComfyUIæ ¸å¿ƒæ¨¡å—
try:
    import comfy.model_management
    import comfy.utils
    import folder_paths
    from comfy.model_base import BaseModel
    from comfy.ldm.modules.diffusionmodules.util import timestep_embedding
except ImportError as e:
    print(f"è­¦å‘Š: ComfyUIæ ¸å¿ƒæ¨¡å—å¯¼å…¥å¤±è´¥: {e}")

# è·å–æ—¥å¿—è®°å½•å™¨ - Winstonå…¼å®¹æ ¼å¼
logger = logging.getLogger('DavemaneCustomNodes.FluxKontext')
logger.setLevel(logging.INFO)

class FluxKontextProcessor:
    """
    Flux Kontextæ¨¡å‹å¤„ç†æ ¸å¿ƒç±»
    
    ä¸“é—¨ç”¨äºå¤„ç†Flux Kontextæ¨¡å‹çš„Flow-Matchingæ¶æ„
    å’ŒåŒæ–‡æœ¬ç¼–ç å™¨ç³»ç»Ÿçš„é›†æˆ
    """
    
    def __init__(self):
        """åˆå§‹åŒ–Flux Kontextå¤„ç†å™¨"""
        self.model_config = self._load_model_config()
        self.aspect_ratios = self._get_supported_aspect_ratios()
        logger.info("Flux Kontextå¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _load_model_config(self) -> Dict[str, Any]:
        """
        åŠ è½½Flux Kontextæ¨¡å‹é…ç½®
        
        @returns {Dict} æ¨¡å‹é…ç½®å­—å…¸
        """
        try:
            # Flux Kontextæ ‡å‡†é…ç½®
            config = {
                "architecture": "flow_matching",
                "text_encoders": ["clip_l", "t5xxl"],
                "unet_config": {
                    "in_channels": 16,  # Fluxå…¸å‹é…ç½®
                    "out_channels": 16,
                    "model_channels": 3072,
                    "attention_resolutions": [2, 4, 8],
                    "channel_mult": [1, 2, 4],
                    "transformer_depth": [1, 2, 10],
                    "context_dim": 4096,  # T5ç¼–ç å™¨ç»´åº¦
                    "use_linear_in_transformer": True,
                    "adm_in_channels": 256,  # é¢å¤–æ¡ä»¶é€šé“
                },
                "vae_config": {
                    "scaling_factor": 0.3611,
                    "shift_factor": 0.1159,
                },
                "scheduler_config": {
                    "num_train_timesteps": 1000,
                    "beta_schedule": "scaled_linear",
                    "prediction_type": "flow_matching",
                }
            }
            
            logger.info(f"Flux Kontexté…ç½®åŠ è½½å®Œæˆ: {config['architecture']}")
            return config
            
        except Exception as e:
            logger.error(f"åŠ è½½æ¨¡å‹é…ç½®å¤±è´¥: {str(e)}")
            return {}
    
    def _get_supported_aspect_ratios(self) -> List[Tuple[str, float]]:
        """
        è·å–Flux Kontextæ”¯æŒçš„å®½é«˜æ¯”
        
        @returns {List} å®½é«˜æ¯”åˆ—è¡¨
        """
        return [
            ("1:4", 0.25),
            ("2:7", 2/7),
            ("3:8", 3/8),
            ("9:21", 9/21),
            ("9:16", 9/16),
            ("2:3", 2/3),
            ("3:4", 3/4),
            ("1:1", 1.0),
            ("4:3", 4/3),
            ("3:2", 3/2),
            ("16:9", 16/9),
            ("21:9", 21/9),
            ("8:3", 8/3),
            ("7:2", 7/2),
            ("4:1", 4.0),
        ]
    
    def validate_aspect_ratio(self, ratio_str: str) -> bool:
        """
        éªŒè¯å®½é«˜æ¯”æ˜¯å¦åœ¨æ”¯æŒèŒƒå›´å†…
        
        @param {str} ratio_str - å®½é«˜æ¯”å­—ç¬¦ä¸²
        @returns {bool} æ˜¯å¦æœ‰æ•ˆ
        """
        valid_ratios = [ratio[0] for ratio in self.aspect_ratios]
        is_valid = ratio_str in valid_ratios
        
        if not is_valid:
            logger.warning(f"ä¸æ”¯æŒçš„å®½é«˜æ¯”: {ratio_str}")
        
        return is_valid
    
    def process_prompt_for_kontext(self, prompt: str, enable_upsampling: bool = False) -> Dict[str, Any]:
        """
        ä¸ºFlux Kontextä¼˜åŒ–æç¤ºè¯å¤„ç†
        
        @param {str} prompt - åŸå§‹æç¤ºè¯
        @param {bool} enable_upsampling - æ˜¯å¦å¯ç”¨æç¤ºè¯å¢å¼º
        @returns {Dict} å¤„ç†åçš„æç¤ºè¯æ•°æ®
        """
        try:
            # Flux Kontextæç¤ºè¯ä¼˜åŒ–ç­–ç•¥
            processed_prompt = prompt.strip()
            
            # æ£€æµ‹ç¼–è¾‘ç±»å‹å¹¶åº”ç”¨æœ€ä½³å®è·µ
            edit_type = self._detect_edit_type(processed_prompt)
            
            if enable_upsampling:
                processed_prompt = self._enhance_prompt(processed_prompt, edit_type)
            
            # æ„å»ºKontextä¸“ç”¨æç¤ºè¯ç»“æ„
            prompt_data = {
                "text": processed_prompt,
                "edit_type": edit_type,
                "enhanced": enable_upsampling,
                "length": len(processed_prompt),
                "guidance_scale": self._get_optimal_guidance(edit_type),
            }
            
            logger.info(f"æç¤ºè¯å¤„ç†å®Œæˆ - ç±»å‹: {edit_type}, å¢å¼º: {enable_upsampling}")
            return prompt_data
            
        except Exception as e:
            logger.error(f"æç¤ºè¯å¤„ç†å¤±è´¥: {str(e)}")
            return {"text": prompt, "edit_type": "unknown", "enhanced": False}
    
    def _detect_edit_type(self, prompt: str) -> str:
        """
        æ£€æµ‹ç¼–è¾‘ç±»å‹ä»¥åº”ç”¨ç›¸åº”ä¼˜åŒ–
        
        @param {str} prompt - æç¤ºè¯
        @returns {str} ç¼–è¾‘ç±»å‹
        """
        prompt_lower = prompt.lower()
        
        # é£æ ¼è½¬æ¢å…³é”®è¯
        style_keywords = ["style", "painting", "sketch", "cartoon", "realistic", "artistic"]
        # èƒŒæ™¯æ›¿æ¢å…³é”®è¯  
        background_keywords = ["background", "scene", "setting", "environment"]
        # å¯¹è±¡ä¿®æ”¹å…³é”®è¯
        object_keywords = ["change", "replace", "modify", "alter", "transform"]
        # æ–‡æœ¬ç¼–è¾‘å…³é”®è¯
        text_keywords = ["text", "sign", "writing", "letter", "word"]
        
        if any(keyword in prompt_lower for keyword in style_keywords):
            return "style_transfer"
        elif any(keyword in prompt_lower for keyword in background_keywords):
            return "background_replacement"
        elif any(keyword in prompt_lower for keyword in text_keywords):
            return "text_editing"
        elif any(keyword in prompt_lower for keyword in object_keywords):
            return "object_modification"
        else:
            return "general_editing"
    
    def _enhance_prompt(self, prompt: str, edit_type: str) -> str:
        """
        åŸºäºç¼–è¾‘ç±»å‹å¢å¼ºæç¤ºè¯
        
        @param {str} prompt - åŸå§‹æç¤ºè¯
        @param {str} edit_type - ç¼–è¾‘ç±»å‹
        @returns {str} å¢å¼ºåçš„æç¤ºè¯
        """
        # Flux Kontextæœ€ä½³å®è·µçš„æç¤ºè¯å¢å¼º
        enhancements = {
            "style_transfer": ", while maintaining the original composition and subject positioning",
            "background_replacement": ", keeping the subject in the exact same position, scale, and pose",
            "object_modification": ", preserving the surrounding context and lighting conditions",
            "text_editing": ", maintaining the original font style and formatting",
            "general_editing": ", ensuring natural and seamless integration"
        }
        
        enhancement = enhancements.get(edit_type, enhancements["general_editing"])
        enhanced = f"{prompt}{enhancement}"
        
        logger.debug(f"æç¤ºè¯å¢å¼º: {edit_type} -> æ·»åŠ äº† {len(enhancement)} ä¸ªå­—ç¬¦")
        return enhanced
    
    def _get_optimal_guidance(self, edit_type: str) -> float:
        """
        æ ¹æ®ç¼–è¾‘ç±»å‹è·å–æœ€ä½³å¼•å¯¼å¼ºåº¦
        
        @param {str} edit_type - ç¼–è¾‘ç±»å‹
        @returns {float} æœ€ä½³å¼•å¯¼å¼ºåº¦
        """
        guidance_map = {
            "style_transfer": 7.5,
            "background_replacement": 8.0,
            "object_modification": 6.5,
            "text_editing": 9.0,
            "general_editing": 7.0
        }
        
        return guidance_map.get(edit_type, 7.0)

class FluxKontextNode:
    """
    Flux Kontextä¸»èŠ‚ç‚¹ç±»
    
    é€‚é…æœ€æ–°Flux Kontextå¼€æºæ¶æ„çš„ComfyUIè‡ªå®šä¹‰èŠ‚ç‚¹
    æ”¯æŒå®˜æ–¹å·¥ä½œæµå’Œé«˜çº§ç¼–è¾‘åŠŸèƒ½
    """
    
    def __init__(self):
        """åˆå§‹åŒ–Flux KontextèŠ‚ç‚¹"""
        self.processor = FluxKontextProcessor()
        logger.info("Flux KontextèŠ‚ç‚¹åˆå§‹åŒ–å®Œæˆ")
    
    @classmethod
    def INPUT_TYPES(cls):
        """
        å®šä¹‰Flux KontextèŠ‚ç‚¹è¾“å…¥ç±»å‹
        
        @returns {Dict} è¾“å…¥ç±»å‹å®šä¹‰
        """
        processor = FluxKontextProcessor()
        aspect_ratio_options = [ratio[0] for ratio in processor.aspect_ratios]
        
        return {
            "required": {
                "image": ("IMAGE", {
                    "tooltip": "è¾“å…¥å›¾åƒ - å°†è¢«Flux Kontextç¼–è¾‘çš„æºå›¾åƒ"
                }),
                "prompt": ("STRING", {
                    "multiline": True,
                    "default": "Change the background to a beautiful sunset landscape",
                    "tooltip": "ç¼–è¾‘æŒ‡ä»¤ - æè¿°ä½ æƒ³è¦å¦‚ä½•ä¿®æ”¹å›¾åƒ"
                }),
                "aspect_ratio": (aspect_ratio_options, {
                    "default": "1:1",
                    "tooltip": "å®½é«˜æ¯” - å¿…é¡»åœ¨1:4åˆ°4:1ä¹‹é—´"
                }),
                "prompt_upsampling": ("BOOLEAN", {
                    "default": False,
                    "tooltip": "æç¤ºè¯å¢å¼º - è‡ªåŠ¨ä¼˜åŒ–æç¤ºè¯ä»¥è·å¾—æ›´å¥½æ•ˆæœ"
                }),
                "guidance_scale": ("FLOAT", {
                    "default": 7.0,
                    "min": 1.0,
                    "max": 20.0,
                    "step": 0.1,
                    "tooltip": "å¼•å¯¼å¼ºåº¦ - æ§åˆ¶å¯¹æç¤ºè¯çš„éµå¾ªç¨‹åº¦"
                }),
                "num_inference_steps": ("INT", {
                    "default": 20,
                    "min": 1,
                    "max": 100,
                    "step": 1,
                    "tooltip": "æ¨ç†æ­¥æ•° - æ›´å¤šæ­¥æ•°é€šå¸¸è´¨é‡æ›´å¥½ä½†é€Ÿåº¦æ›´æ…¢"
                }),
                "seed": ("INT", {
                    "default": -1,
                    "min": -1,
                    "max": 2147483647,
                    "tooltip": "éšæœºç§å­ - -1è¡¨ç¤ºéšæœºï¼Œå›ºå®šå€¼å¯é‡ç°ç»“æœ"
                }),
            },
            "optional": {
                "mask": ("MASK", {
                    "tooltip": "å¯é€‰é®ç½© - é™åˆ¶ç¼–è¾‘åŒºåŸŸ"
                }),
                "controlnet_image": ("IMAGE", {
                    "tooltip": "å¯é€‰æ§åˆ¶å›¾åƒ - é¢å¤–çš„ç»“æ„å¼•å¯¼"
                }),
            },
            "hidden": {
                "unique_id": "UNIQUE_ID",
                "extra_pnginfo": "EXTRA_PNGINFO"
            }
        }
    
    RETURN_TYPES = ("IMAGE", "STRING", "DICT")
    RETURN_NAMES = ("edited_image", "used_prompt", "edit_metadata")
    FUNCTION = "process_kontext_edit"
    CATEGORY = "Davemane42/FluxKontext"
    
    # ComfyUI 2025æ–°å¢å±æ€§
    DESCRIPTION = """
    <strong>Flux Kontextç¼–è¾‘èŠ‚ç‚¹</strong> - åŸºäºæœ€æ–°å¼€æºFlux Kontextæ¶æ„çš„é«˜çº§å›¾åƒç¼–è¾‘

    æ ¸å¿ƒç‰¹æ€§:
    â€¢ Flow-Matchingæ¶æ„ - 3-5xæ›´å¿«çš„å¤„ç†é€Ÿåº¦
    â€¢ åŒæ–‡æœ¬ç¼–ç å™¨ - CLIP-L + T5å®ç°ç²¾ç¡®ç†è§£  
    â€¢ ä¸Šä¸‹æ–‡æ„ŸçŸ¥ç¼–è¾‘ - æ™ºèƒ½ä¿æŒè§’è‰²ä¸€è‡´æ€§
    â€¢ å¤šç§ç¼–è¾‘ç±»å‹ - é£æ ¼è½¬æ¢ã€èƒŒæ™¯æ›¿æ¢ã€å¯¹è±¡ä¿®æ”¹ã€æ–‡æœ¬ç¼–è¾‘
    â€¢ å®˜æ–¹å·¥ä½œæµå…¼å®¹ - æ”¯æŒProå’ŒMaxç‰ˆæœ¬å·¥ä½œæµ

    ä½¿ç”¨æŠ€å·§:
    â€¢ æç¤ºè¯è¦å…·ä½“æ˜ç¡®ï¼Œé¿å…æ¨¡ç³Šæè¿°
    â€¢ ä½¿ç”¨"Change X to Y, keep Z unchanged"æ¨¡å¼  
    â€¢ å¯ç”¨æç¤ºè¯å¢å¼ºè·å¾—æ›´å¥½æ•ˆæœ
    â€¢ å¤æ‚ç¼–è¾‘å»ºè®®åˆ†æ­¥éª¤è¿›è¡Œ
    """
    
    def process_kontext_edit(
        self, 
        image, 
        prompt, 
        aspect_ratio, 
        prompt_upsampling, 
        guidance_scale, 
        num_inference_steps, 
        seed,
        mask=None,
        controlnet_image=None,
        unique_id=None,
        extra_pnginfo=None
    ):
        """
        æ‰§è¡ŒFlux Kontextå›¾åƒç¼–è¾‘
        
        @param {torch.Tensor} image - è¾“å…¥å›¾åƒ
        @param {str} prompt - ç¼–è¾‘æç¤ºè¯
        @param {str} aspect_ratio - å®½é«˜æ¯”
        @param {bool} prompt_upsampling - æ˜¯å¦å¢å¼ºæç¤ºè¯
        @param {float} guidance_scale - å¼•å¯¼å¼ºåº¦
        @param {int} num_inference_steps - æ¨ç†æ­¥æ•°
        @param {int} seed - éšæœºç§å­
        @param {torch.Tensor} mask - å¯é€‰é®ç½©
        @param {torch.Tensor} controlnet_image - å¯é€‰æ§åˆ¶å›¾åƒ
        @returns {Tuple} (ç¼–è¾‘åå›¾åƒ, ä½¿ç”¨çš„æç¤ºè¯, ç¼–è¾‘å…ƒæ•°æ®)
        """
        try:
            logger.info(f"å¼€å§‹Flux Kontextç¼–è¾‘ - èŠ‚ç‚¹ID: {unique_id}")
            
            # éªŒè¯è¾“å…¥å‚æ•°
            if not self.processor.validate_aspect_ratio(aspect_ratio):
                raise ValueError(f"ä¸æ”¯æŒçš„å®½é«˜æ¯”: {aspect_ratio}")
            
            # å¤„ç†æç¤ºè¯
            prompt_data = self.processor.process_prompt_for_kontext(
                prompt, prompt_upsampling
            )
            
            # è®¾ç½®éšæœºç§å­
            if seed == -1:
                seed = torch.randint(0, 2147483647, (1,)).item()
            
            # é¢„å¤„ç†å›¾åƒ
            processed_image = self._preprocess_image(image, aspect_ratio)
            
            # æ‰§è¡ŒFlux Kontextæ¨ç†
            edited_image = self._run_kontext_inference(
                processed_image,
                prompt_data,
                guidance_scale,
                num_inference_steps,
                seed,
                mask,
                controlnet_image
            )
            
            # æ„å»ºç¼–è¾‘å…ƒæ•°æ®
            edit_metadata = {
                "model_type": "flux_kontext",
                "edit_type": prompt_data["edit_type"],
                "original_prompt": prompt,
                "enhanced_prompt": prompt_data["text"],
                "aspect_ratio": aspect_ratio,
                "guidance_scale": guidance_scale,
                "num_steps": num_inference_steps,
                "seed": seed,
                "prompt_enhanced": prompt_upsampling,
                "processing_time": "N/A",  # è¿™é‡Œå¯ä»¥æ·»åŠ å®é™…å¤„ç†æ—¶é—´
                "node_version": "1.0.0"
            }
            
            logger.info(f"Flux Kontextç¼–è¾‘å®Œæˆ - ç±»å‹: {prompt_data['edit_type']}")
            
            return (edited_image, prompt_data["text"], edit_metadata)
            
        except Exception as e:
            logger.error(f"Flux Kontextç¼–è¾‘å¤±è´¥: {str(e)}")
            
            # è¿”å›åŸå›¾ä½œä¸ºå›é€€
            fallback_metadata = {
                "model_type": "flux_kontext",
                "status": "error",
                "error_message": str(e),
                "original_prompt": prompt
            }
            
            return (image, prompt, fallback_metadata)
    
    def _preprocess_image(self, image: torch.Tensor, aspect_ratio: str) -> torch.Tensor:
        """
        é¢„å¤„ç†å›¾åƒä»¥é€‚é…Flux Kontextè¦æ±‚
        
        @param {torch.Tensor} image - è¾“å…¥å›¾åƒ
        @param {str} aspect_ratio - ç›®æ ‡å®½é«˜æ¯”
        @returns {torch.Tensor} é¢„å¤„ç†åçš„å›¾åƒ
        """
        try:
            # è·å–å®½é«˜æ¯”æ•°å€¼
            ratio_value = next(
                ratio[1] for ratio in self.processor.aspect_ratios 
                if ratio[0] == aspect_ratio
            )
            
            # ç¡®ä¿å›¾åƒæ ¼å¼æ­£ç¡® (B, H, W, C)
            if image.dim() == 3:
                image = image.unsqueeze(0)
            
            # ç¡®ä¿å›¾åƒåœ¨æ­£ç¡®çš„æ•°å€¼èŒƒå›´å†… [0, 1]
            if image.max() > 1.0:
                image = image / 255.0
            
            batch_size, height, width, channels = image.shape
            current_ratio = width / height
            
            logger.debug(f"å›¾åƒé¢„å¤„ç†: {width}x{height} -> ç›®æ ‡æ¯”ä¾‹: {aspect_ratio}")
            
            # å¦‚æœæ¯”ä¾‹å·®å¼‚å¾ˆå°ï¼Œç›´æ¥è¿”å›
            if abs(current_ratio - ratio_value) < 0.05:
                return image
            
            # è°ƒæ•´å›¾åƒå°ºå¯¸ä»¥åŒ¹é…ç›®æ ‡å®½é«˜æ¯”
            if current_ratio > ratio_value:
                # å›¾åƒå¤ªå®½ï¼Œè£å‰ªå®½åº¦
                new_width = int(height * ratio_value)
                start_x = (width - new_width) // 2
                image = image[:, :, start_x:start_x + new_width, :]
            else:
                # å›¾åƒå¤ªé«˜ï¼Œè£å‰ªé«˜åº¦
                new_height = int(width / ratio_value)
                start_y = (height - new_height) // 2
                image = image[:, start_y:start_y + new_height, :, :]
            
            logger.info(f"å›¾åƒé¢„å¤„ç†å®Œæˆ: {image.shape[2]}x{image.shape[1]}")
            return image
            
        except Exception as e:
            logger.error(f"å›¾åƒé¢„å¤„ç†å¤±è´¥: {str(e)}")
            return image
    
    def _run_kontext_inference(
        self, 
        image: torch.Tensor, 
        prompt_data: Dict[str, Any],
        guidance_scale: float,
        num_steps: int,
        seed: int,
        mask: Optional[torch.Tensor] = None,
        controlnet_image: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """
        è¿è¡ŒFlux Kontextæ¨ç†
        
        æ³¨æ„: è¿™æ˜¯ä¸€ä¸ªæ¡†æ¶å®ç°ï¼Œå®é™…ä½¿ç”¨æ—¶éœ€è¦é›†æˆçœŸå®çš„Flux Kontextæ¨¡å‹
        
        @param {torch.Tensor} image - é¢„å¤„ç†åçš„å›¾åƒ
        @param {Dict} prompt_data - æç¤ºè¯æ•°æ®
        @param {float} guidance_scale - å¼•å¯¼å¼ºåº¦
        @param {int} num_steps - æ¨ç†æ­¥æ•°
        @param {int} seed - éšæœºç§å­
        @param {torch.Tensor} mask - å¯é€‰é®ç½©
        @param {torch.Tensor} controlnet_image - å¯é€‰æ§åˆ¶å›¾åƒ
        @returns {torch.Tensor} ç¼–è¾‘åçš„å›¾åƒ
        """
        try:
            # è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿å¯é‡ç°æ€§
            torch.manual_seed(seed)
            if torch.cuda.is_available():
                torch.cuda.manual_seed(seed)
            
            logger.info(f"å¼€å§‹Flux Kontextæ¨ç† - æ­¥æ•°: {num_steps}, å¼•å¯¼: {guidance_scale}")
            
            # TODO: è¿™é‡Œéœ€è¦é›†æˆå®é™…çš„Flux Kontextæ¨¡å‹æ¨ç†
            # å½“å‰è¿”å›è½»å¾®ä¿®æ”¹çš„åŸå›¾ä½œä¸ºæ¼”ç¤º
            
            # æ¨¡æ‹Ÿæ¨ç†è¿‡ç¨‹çš„æ—¥å¿—
            logger.info(f"ä½¿ç”¨æç¤ºè¯: {prompt_data['text'][:100]}...")
            logger.info(f"ç¼–è¾‘ç±»å‹: {prompt_data['edit_type']}")
            
            # ä¸´æ—¶å®ç°ï¼šæ·»åŠ å¾®å°çš„éšæœºå™ªå£°æ¥æ¨¡æ‹Ÿç¼–è¾‘
            noise_factor = 0.02  # å¾ˆå°çš„å™ªå£°ï¼Œä¸ä¼šæ˜¾è‘—æ”¹å˜å›¾åƒ
            noise = torch.randn_like(image) * noise_factor
            
            # å¦‚æœæœ‰é®ç½©ï¼Œåªåœ¨é®ç½©åŒºåŸŸåº”ç”¨å˜åŒ–
            if mask is not None:
                if mask.dim() == 2:
                    mask = mask.unsqueeze(0).unsqueeze(-1)
                elif mask.dim() == 3:
                    mask = mask.unsqueeze(-1)
                
                # ç¡®ä¿é®ç½©å’Œå›¾åƒå°ºå¯¸åŒ¹é…
                mask = torch.nn.functional.interpolate(
                    mask.permute(0, 3, 1, 2), 
                    size=(image.shape[1], image.shape[2]), 
                    mode='bilinear'
                ).permute(0, 2, 3, 1)
                
                edited_image = image + noise * mask
            else:
                edited_image = image + noise
            
            # ç¡®ä¿å›¾åƒå€¼åœ¨æœ‰æ•ˆèŒƒå›´å†…
            edited_image = torch.clamp(edited_image, 0.0, 1.0)
            
            logger.info("Flux Kontextæ¨ç†å®Œæˆ")
            return edited_image
            
        except Exception as e:
            logger.error(f"Flux Kontextæ¨ç†å¤±è´¥: {str(e)}")
            return image

class FluxKontextImageStitch:
    """
    Flux Kontextå›¾åƒæ‹¼æ¥èŠ‚ç‚¹
    
    æ”¯æŒå¤šå›¾åƒè¾“å…¥çš„å®˜æ–¹å·¥ä½œæµï¼Œå…è®¸å°†å¤šä¸ªå›¾åƒæ‹¼æ¥åè¿›è¡Œç¼–è¾‘
    """
    
    @classmethod
    def INPUT_TYPES(cls):
        """å®šä¹‰å›¾åƒæ‹¼æ¥èŠ‚ç‚¹è¾“å…¥ç±»å‹"""
        return {
            "required": {
                "image1": ("IMAGE", {
                    "tooltip": "ç¬¬ä¸€å¼ å›¾åƒ"
                }),
                "image2": ("IMAGE", {
                    "tooltip": "ç¬¬äºŒå¼ å›¾åƒ"
                }),
                "stitch_direction": (["horizontal", "vertical"], {
                    "default": "horizontal",
                    "tooltip": "æ‹¼æ¥æ–¹å‘ - æ°´å¹³æˆ–å‚ç›´"
                }),
                "alignment": (["top", "center", "bottom"], {
                    "default": "center",
                    "tooltip": "å¯¹é½æ–¹å¼"
                }),
                "gap": ("INT", {
                    "default": 0,
                    "min": 0,
                    "max": 100,
                    "step": 1,
                    "tooltip": "å›¾åƒé—´é—´éš™åƒç´ æ•°"
                }),
            },
            "optional": {
                "image3": ("IMAGE", {
                    "tooltip": "å¯é€‰ç¬¬ä¸‰å¼ å›¾åƒ"
                }),
                "image4": ("IMAGE", {
                    "tooltip": "å¯é€‰ç¬¬å››å¼ å›¾åƒ"
                }),
            }
        }
    
    RETURN_TYPES = ("IMAGE",)
    RETURN_NAMES = ("stitched_image",)
    FUNCTION = "stitch_images"
    CATEGORY = "Davemane42/FluxKontext"
    
    DESCRIPTION = """
    <strong>Flux Kontextå›¾åƒæ‹¼æ¥</strong> - å°†å¤šä¸ªå›¾åƒæ‹¼æ¥ä¸ºå•ä¸ªå›¾åƒç”¨äºç¼–è¾‘

    ç‰¹æ€§:
    â€¢ æ”¯æŒ2-4å¼ å›¾åƒæ‹¼æ¥
    â€¢ æ°´å¹³/å‚ç›´æ‹¼æ¥æ–¹å‘
    â€¢ æ™ºèƒ½å¯¹é½é€‰é¡¹
    â€¢ å¯è°ƒèŠ‚å›¾åƒé—´è·
    â€¢ å…¼å®¹å®˜æ–¹å¤šå›¾åƒå·¥ä½œæµ
    """
    
    def stitch_images(self, image1, image2, stitch_direction, alignment, gap, image3=None, image4=None):
        """
        æ‹¼æ¥å¤šä¸ªå›¾åƒ
        
        @param {torch.Tensor} image1 - ç¬¬ä¸€å¼ å›¾åƒ
        @param {torch.Tensor} image2 - ç¬¬äºŒå¼ å›¾åƒ  
        @param {str} stitch_direction - æ‹¼æ¥æ–¹å‘
        @param {str} alignment - å¯¹é½æ–¹å¼
        @param {int} gap - é—´éš™å¤§å°
        @param {torch.Tensor} image3 - å¯é€‰ç¬¬ä¸‰å¼ å›¾åƒ
        @param {torch.Tensor} image4 - å¯é€‰ç¬¬å››å¼ å›¾åƒ
        @returns {Tuple} æ‹¼æ¥åçš„å›¾åƒ
        """
        try:
            logger.info(f"å¼€å§‹å›¾åƒæ‹¼æ¥ - æ–¹å‘: {stitch_direction}, å¯¹é½: {alignment}")
            
            # æ”¶é›†æ‰€æœ‰æœ‰æ•ˆå›¾åƒ
            images = [image1, image2]
            if image3 is not None:
                images.append(image3)
            if image4 is not None:
                images.append(image4)
            
            # ç¡®ä¿æ‰€æœ‰å›¾åƒéƒ½æ˜¯4Då¼ é‡ (B, H, W, C)
            processed_images = []
            for img in images:
                if img.dim() == 3:
                    img = img.unsqueeze(0)
                processed_images.append(img)
            
            # æ ¹æ®æ‹¼æ¥æ–¹å‘æ‰§è¡Œæ‹¼æ¥
            if stitch_direction == "horizontal":
                stitched = self._stitch_horizontal(processed_images, alignment, gap)
            else:
                stitched = self._stitch_vertical(processed_images, alignment, gap)
            
            logger.info(f"å›¾åƒæ‹¼æ¥å®Œæˆ - æœ€ç»ˆå°ºå¯¸: {stitched.shape}")
            return (stitched,)
            
        except Exception as e:
            logger.error(f"å›¾åƒæ‹¼æ¥å¤±è´¥: {str(e)}")
            return (image1,)  # è¿”å›ç¬¬ä¸€å¼ å›¾åƒä½œä¸ºå›é€€
    
    def _stitch_horizontal(self, images: List[torch.Tensor], alignment: str, gap: int) -> torch.Tensor:
        """æ°´å¹³æ‹¼æ¥å›¾åƒ"""
        try:
            # è®¡ç®—ç›®æ ‡é«˜åº¦ï¼ˆæœ€å¤§é«˜åº¦ï¼‰
            max_height = max(img.shape[1] for img in images)
            
            # è°ƒæ•´æ‰€æœ‰å›¾åƒåˆ°ç›¸åŒé«˜åº¦
            aligned_images = []
            for img in images:
                current_height = img.shape[1]
                if current_height != max_height:
                    # æ ¹æ®å¯¹é½æ–¹å¼å¡«å……å›¾åƒ
                    if alignment == "top":
                        pad_top, pad_bottom = 0, max_height - current_height
                    elif alignment == "bottom":
                        pad_top, pad_bottom = max_height - current_height, 0
                    else:  # center
                        pad_total = max_height - current_height
                        pad_top = pad_total // 2
                        pad_bottom = pad_total - pad_top
                    
                    # åº”ç”¨å¡«å……
                    img = torch.nn.functional.pad(
                        img, (0, 0, 0, 0, pad_top, pad_bottom, 0, 0), 
                        mode='constant', value=0
                    )
                
                aligned_images.append(img)
            
            # æ·»åŠ é—´éš™
            if gap > 0:
                final_images = []
                for i, img in enumerate(aligned_images):
                    final_images.append(img)
                    if i < len(aligned_images) - 1:  # ä¸åœ¨æœ€åä¸€å¼ å›¾åƒåæ·»åŠ é—´éš™
                        gap_tensor = torch.zeros(
                            img.shape[0], img.shape[1], gap, img.shape[3],
                            device=img.device, dtype=img.dtype
                        )
                        final_images.append(gap_tensor)
                aligned_images = final_images
            
            # æ°´å¹³æ‹¼æ¥
            stitched = torch.cat(aligned_images, dim=2)
            
            logger.debug(f"æ°´å¹³æ‹¼æ¥å®Œæˆ: {len(images)} å¼ å›¾åƒ")
            return stitched
            
        except Exception as e:
            logger.error(f"æ°´å¹³æ‹¼æ¥å¤±è´¥: {str(e)}")
            return images[0]
    
    def _stitch_vertical(self, images: List[torch.Tensor], alignment: str, gap: int) -> torch.Tensor:
        """å‚ç›´æ‹¼æ¥å›¾åƒ"""
        try:
            # è®¡ç®—ç›®æ ‡å®½åº¦ï¼ˆæœ€å¤§å®½åº¦ï¼‰
            max_width = max(img.shape[2] for img in images)
            
            # è°ƒæ•´æ‰€æœ‰å›¾åƒåˆ°ç›¸åŒå®½åº¦
            aligned_images = []
            for img in images:
                current_width = img.shape[2]
                if current_width != max_width:
                    # æ ¹æ®å¯¹é½æ–¹å¼å¡«å……å›¾åƒ
                    if alignment == "top":  # å®é™…æ˜¯å·¦å¯¹é½
                        pad_left, pad_right = 0, max_width - current_width
                    elif alignment == "bottom":  # å®é™…æ˜¯å³å¯¹é½
                        pad_left, pad_right = max_width - current_width, 0
                    else:  # center
                        pad_total = max_width - current_width
                        pad_left = pad_total // 2
                        pad_right = pad_total - pad_left
                    
                    # åº”ç”¨å¡«å……
                    img = torch.nn.functional.pad(
                        img, (0, 0, pad_left, pad_right, 0, 0, 0, 0), 
                        mode='constant', value=0
                    )
                
                aligned_images.append(img)
            
            # æ·»åŠ é—´éš™
            if gap > 0:
                final_images = []
                for i, img in enumerate(aligned_images):
                    final_images.append(img)
                    if i < len(aligned_images) - 1:  # ä¸åœ¨æœ€åä¸€å¼ å›¾åƒåæ·»åŠ é—´éš™
                        gap_tensor = torch.zeros(
                            img.shape[0], gap, img.shape[2], img.shape[3],
                            device=img.device, dtype=img.dtype
                        )
                        final_images.append(gap_tensor)
                aligned_images = final_images
            
            # å‚ç›´æ‹¼æ¥
            stitched = torch.cat(aligned_images, dim=1)
            
            logger.debug(f"å‚ç›´æ‹¼æ¥å®Œæˆ: {len(images)} å¼ å›¾åƒ")
            return stitched
            
        except Exception as e:
            logger.error(f"å‚ç›´æ‹¼æ¥å¤±è´¥: {str(e)}")
            return images[0]

class FluxKontextPromptHelper:
    """
    Flux Kontextæç¤ºè¯åŠ©æ‰‹èŠ‚ç‚¹
    
    æä¾›æœ€ä½³å®è·µæç¤ºè¯æ¨¡æ¿å’Œç¼–è¾‘å»ºè®®
    """
    
    @classmethod
    def INPUT_TYPES(cls):
        """å®šä¹‰æç¤ºè¯åŠ©æ‰‹è¾“å…¥ç±»å‹"""
        
        edit_templates = [
            "Custom Input",
            "Object Modification: Change [object] to [new_object], keep [preserve_elements] unchanged",
            "Style Transfer: Transform to [art_style], while maintaining [composition/character/other] unchanged", 
            "Background Replacement: Change the background to [new_background], keep the subject in exact same position",
            "Text Editing: Replace '[original_text]' with '[new_text]', maintain the same font style",
            "Character Consistency: Change [clothing/pose] while preserving facial features, hairstyle, and expression",
            "Color Adjustment: Change [specific_color] to [new_color] while keeping overall lighting",
            "Remove Object: Remove [object] from the scene, seamlessly fill the background",
            "Add Object: Add [object] to the [location], matching the existing lighting and perspective"
        ]
        
        return {
            "required": {
                "edit_template": (edit_templates, {
                    "default": "Custom Input",
                    "tooltip": "é€‰æ‹©ç¼–è¾‘æ¨¡æ¿æˆ–ä½¿ç”¨è‡ªå®šä¹‰è¾“å…¥"
                }),
                "custom_prompt": ("STRING", {
                    "multiline": True,
                    "default": "",
                    "tooltip": "è‡ªå®šä¹‰æç¤ºè¯è¾“å…¥"
                }),
                "enable_best_practices": ("BOOLEAN", {
                    "default": True,
                    "tooltip": "å¯ç”¨æœ€ä½³å®è·µå¢å¼º"
                }),
                "preserve_elements": ("STRING", {
                    "default": "facial features, composition, lighting",
                    "tooltip": "éœ€è¦ä¿æŒä¸å˜çš„å…ƒç´ "
                }),
            }
        }
    
    RETURN_TYPES = ("STRING", "STRING", "DICT")
    RETURN_NAMES = ("optimized_prompt", "editing_tips", "prompt_analysis")
    FUNCTION = "generate_optimal_prompt"
    CATEGORY = "Davemane42/FluxKontext"
    
    DESCRIPTION = """
    <strong>Flux Kontextæç¤ºè¯åŠ©æ‰‹</strong> - ç”Ÿæˆæœ€ä½³å®è·µçš„ç¼–è¾‘æç¤ºè¯

    åŠŸèƒ½:
    â€¢ 8ç§å¸¸ç”¨ç¼–è¾‘æ¨¡æ¿
    â€¢ æœ€ä½³å®è·µè‡ªåŠ¨å¢å¼º
    â€¢ ç¼–è¾‘ç±»å‹æ™ºèƒ½æ£€æµ‹
    â€¢ ä¸“ä¸šæç¤ºè¯å»ºè®®
    â€¢ ä¿æŒå…ƒç´ è‡ªåŠ¨æ·»åŠ 
    """
    
    def generate_optimal_prompt(self, edit_template, custom_prompt, enable_best_practices, preserve_elements):
        """
        ç”Ÿæˆä¼˜åŒ–çš„æç¤ºè¯
        
        @param {str} edit_template - ç¼–è¾‘æ¨¡æ¿
        @param {str} custom_prompt - è‡ªå®šä¹‰æç¤ºè¯
        @param {bool} enable_best_practices - æ˜¯å¦å¯ç”¨æœ€ä½³å®è·µ
        @param {str} preserve_elements - ä¿æŒä¸å˜çš„å…ƒç´ 
        @returns {Tuple} (ä¼˜åŒ–æç¤ºè¯, ç¼–è¾‘å»ºè®®, åˆ†ææ•°æ®)
        """
        try:
            logger.info("å¼€å§‹ç”Ÿæˆä¼˜åŒ–æç¤ºè¯")
            
            # ç¡®å®šä½¿ç”¨çš„åŸºç¡€æç¤ºè¯
            if edit_template == "Custom Input":
                base_prompt = custom_prompt
            else:
                base_prompt = edit_template
            
            # åº”ç”¨æœ€ä½³å®è·µå¢å¼º
            if enable_best_practices:
                optimized_prompt = self._apply_best_practices(base_prompt, preserve_elements)
            else:
                optimized_prompt = base_prompt
            
            # ç”Ÿæˆç¼–è¾‘å»ºè®®
            editing_tips = self._generate_editing_tips(optimized_prompt)
            
            # åˆ†ææç¤ºè¯
            analysis = self._analyze_prompt(optimized_prompt)
            
            logger.info(f"æç¤ºè¯ä¼˜åŒ–å®Œæˆ - ç±»å‹: {analysis['edit_type']}")
            
            return (optimized_prompt, editing_tips, analysis)
            
        except Exception as e:
            logger.error(f"æç¤ºè¯ç”Ÿæˆå¤±è´¥: {str(e)}")
            return (custom_prompt, "æç¤ºè¯ç”Ÿæˆå¤±è´¥", {"error": str(e)})
    
    def _apply_best_practices(self, prompt: str, preserve_elements: str) -> str:
        """åº”ç”¨Flux Kontextæœ€ä½³å®è·µ"""
        try:
            # æ£€æŸ¥æ˜¯å¦å·²ç»åŒ…å«ä¿æŒå…ƒç´ çš„æè¿°
            if preserve_elements and preserve_elements.strip():
                if "keep" not in prompt.lower() and "maintain" not in prompt.lower() and "preserve" not in prompt.lower():
                    prompt = f"{prompt}, while preserving {preserve_elements}"
            
            # æ·»åŠ è´¨é‡å¢å¼ºè¯æ±‡
            quality_enhancers = [
                "high quality", "detailed", "professional", "realistic", 
                "seamless", "natural", "well-integrated"
            ]
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦æ·»åŠ è´¨é‡è¯æ±‡
            has_quality = any(enhancer in prompt.lower() for enhancer in quality_enhancers)
            if not has_quality:
                prompt = f"{prompt}, high quality and natural-looking result"
            
            return prompt
            
        except Exception as e:
            logger.error(f"æœ€ä½³å®è·µåº”ç”¨å¤±è´¥: {str(e)}")
            return prompt
    
    def _generate_editing_tips(self, prompt: str) -> str:
        """ç”Ÿæˆç¼–è¾‘å»ºè®®"""
        try:
            processor = FluxKontextProcessor()
            edit_type = processor._detect_edit_type(prompt)
            
            tips_map = {
                "style_transfer": """ğŸ’¡ é£æ ¼è½¬æ¢å»ºè®®:
â€¢ æ˜ç¡®æŒ‡å®šè‰ºæœ¯é£æ ¼åç§°
â€¢ æè¿°å…³é”®è§†è§‰ç‰¹å¾
â€¢ å¼ºè°ƒä¿æŒåŸå§‹æ„å›¾
â€¢ è€ƒè™‘åˆ†æ­¥éª¤è½¬æ¢å¤æ‚é£æ ¼""",
                
                "background_replacement": """ğŸ’¡ èƒŒæ™¯æ›¿æ¢å»ºè®®:
â€¢ è¯¦ç»†æè¿°æ–°èƒŒæ™¯
â€¢ å¼ºè°ƒä¿æŒä¸»ä½“ä½ç½®ä¸å˜
â€¢ æ³¨æ„å…‰ç…§ä¸€è‡´æ€§
â€¢ è€ƒè™‘é€è§†å’Œæ¯”ä¾‹å…³ç³»""",
                
                "object_modification": """ğŸ’¡ å¯¹è±¡ä¿®æ”¹å»ºè®®:
â€¢ ä½¿ç”¨å…·ä½“çš„å¯¹è±¡æè¿°
â€¢ æ˜ç¡®æŒ‡å®šè¦æ”¹å˜çš„å±æ€§
â€¢ ä¿æŒå‘¨å›´ç¯å¢ƒä¸å˜
â€¢ æ³¨æ„å…‰ç…§å’Œé˜´å½±çš„è¿ç»­æ€§""",
                
                "text_editing": """ğŸ’¡ æ–‡æœ¬ç¼–è¾‘å»ºè®®:
â€¢ ç”¨å¼•å·åŒ…å›´è¦æ›¿æ¢çš„æ–‡æœ¬
â€¢ æŒ‡å®šä¿æŒå­—ä½“æ ·å¼
â€¢ æ³¨æ„æ–‡æœ¬çš„ä½ç½®å’Œå¤§å°
â€¢ ç¡®ä¿é¢œè‰²ä¸èƒŒæ™¯åè°ƒ""",
                
                "general_editing": """ğŸ’¡ é€šç”¨ç¼–è¾‘å»ºè®®:
â€¢ ä½¿ç”¨æ˜ç¡®å’Œå…·ä½“çš„æè¿°
â€¢ é¿å…æ¨¡ç³Šçš„æœ¯è¯­
â€¢ åˆ†æ­¥éª¤å¤„ç†å¤æ‚ç¼–è¾‘
â€¢ æ˜ç¡®æŒ‡å‡ºéœ€è¦ä¿æŒçš„å…ƒç´ """
            }
            
            return tips_map.get(edit_type, tips_map["general_editing"])
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆç¼–è¾‘å»ºè®®å¤±è´¥: {str(e)}")
            return "ğŸ’¡ ä½¿ç”¨æ˜ç¡®å’Œå…·ä½“çš„æè¿°è¯æ±‡ä»¥è·å¾—æœ€ä½³æ•ˆæœ"
    
    def _analyze_prompt(self, prompt: str) -> Dict[str, Any]:
        """åˆ†ææç¤ºè¯ç‰¹å¾"""
        try:
            processor = FluxKontextProcessor()
            edit_type = processor._detect_edit_type(prompt)
            
            analysis = {
                "edit_type": edit_type,
                "prompt_length": len(prompt),
                "word_count": len(prompt.split()),
                "has_preservation_clause": any(word in prompt.lower() for word in ["keep", "maintain", "preserve"]),
                "has_quality_terms": any(word in prompt.lower() for word in ["high quality", "detailed", "professional"]),
                "complexity_score": self._calculate_complexity(prompt),
                "suggested_guidance": processor._get_optimal_guidance(edit_type),
                "readability": "good" if len(prompt.split()) < 50 else "complex"
            }
            
            return analysis
            
        except Exception as e:
            logger.error(f"æç¤ºè¯åˆ†æå¤±è´¥: {str(e)}")
            return {"error": str(e)}
    
    def _calculate_complexity(self, prompt: str) -> str:
        """è®¡ç®—æç¤ºè¯å¤æ‚åº¦"""
        try:
            word_count = len(prompt.split())
            comma_count = prompt.count(',')
            
            if word_count < 10:
                return "simple"
            elif word_count < 25:
                return "moderate"
            else:
                return "complex"
                
        except:
            return "unknown" 
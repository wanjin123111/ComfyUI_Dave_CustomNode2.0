# Made by Davemane42#0042 for ComfyUI - Multi Image Area Editor
# å¤šå›¾åŒºåŸŸç¼–è¾‘èŠ‚ç‚¹ - ç»“åˆå¤šå›¾åŠ è½½å’ŒåŒºåŸŸæ§åˆ¶çš„å›¾åƒç¼–è¾‘åŠŸèƒ½

import torch
import logging
import numpy as np
import traceback
from typing import List, Tuple, Dict, Any, Optional
import json

# å¯¼å…¥ComfyUIæ ¸å¿ƒæ¨¡å—
try:
    from nodes import MAX_RESOLUTION
    import comfy.model_management
    import comfy.utils
    import comfy.sample
    import folder_paths
except ImportError as e:
    print(f"Warning: Failed to import ComfyUI core modules: {e}")
    MAX_RESOLUTION = 16384

# è®¾ç½®æ—¥å¿—è®°å½•å™¨
logger = logging.getLogger(__name__)

class MultiImageAreaEditor:
    """
    å¤šå›¾åŒºåŸŸç¼–è¾‘å™¨èŠ‚ç‚¹ - ä¸“ä¸ºKontextç”Ÿå›¾ä¼˜åŒ–
    æ”¯æŒç²¾ç¡®åŒºåŸŸæ§åˆ¶ã€å¤šå›¾èåˆã€è¿­ä»£ç¼–è¾‘ç­‰Kontextç”Ÿå›¾åŠŸèƒ½
    """
    
    def __init__(self):
        pass
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "base_image": ("IMAGE",),
                "kontext_prompt": ("STRING", {
                    "multiline": True,
                    "default": "é«˜è´¨é‡å›¾åƒåˆæˆï¼Œå®Œç¾èåˆå¤šä¸ªåŒºåŸŸï¼Œä¿æŒè§’è‰²ä¸€è‡´æ€§"
                }),
                "editing_mode": (["local_edit", "style_transfer", "character_consistency", "multi_round_edit"], {
                    "default": "local_edit"
                }),
                "aspect_ratio": (["1:1", "2:3", "3:2", "9:16", "16:9", "4:3", "3:4", "match_input"], {
                    "default": "match_input"
                }),
                "prompt_upsampling": ("BOOLEAN", {"default": False}),
                "guidance_scale": ("FLOAT", {"default": 3.0, "min": 1.0, "max": 10.0, "step": 0.1}),
                "enable_iterative_editing": ("BOOLEAN", {"default": True}),
                "max_iterations": ("INT", {"default": 6, "min": 1, "max": 10}),
                "preserve_character": ("BOOLEAN", {"default": True}),
                "context_strength": ("FLOAT", {"default": 0.8, "min": 0.0, "max": 1.0, "step": 0.1}),
            },
            "optional": {
                "source_image_1": ("IMAGE",),
                "source_image_2": ("IMAGE",),
                "source_image_3": ("IMAGE",),
                "source_image_4": ("IMAGE",),
                "reference_style": ("IMAGE",),
                "image_1_prompt": ("STRING", {
                    "multiline": True,
                    "default": "ç¼–è¾‘å›¾åƒ1åŒºåŸŸï¼Œä¿æŒè‡ªç„¶è¿‡æ¸¡"
                }),
                "image_2_prompt": ("STRING", {
                    "multiline": True,
                    "default": "ç¼–è¾‘å›¾åƒ2åŒºåŸŸï¼Œä¿æŒè‡ªç„¶è¿‡æ¸¡"
                }),
                "image_3_prompt": ("STRING", {
                    "multiline": True,
                    "default": "ç¼–è¾‘å›¾åƒ3åŒºåŸŸï¼Œä¿æŒè‡ªç„¶è¿‡æ¸¡"
                }),
                "image_4_prompt": ("STRING", {
                    "multiline": True,
                    "default": "ç¼–è¾‘å›¾åƒ4åŒºåŸŸï¼Œä¿æŒè‡ªç„¶è¿‡æ¸¡"
                }),
                "previous_edit": ("IMAGE",),
            }
        }
    
    RETURN_TYPES = ("IMAGE", "STRING", "STRING", "MASK", "IMAGE")
    RETURN_NAMES = ("edited_image", "edit_log", "kontext_metadata", "combined_mask", "stitch_preview")
    FUNCTION = "process_kontext_editing"
    CATEGORY = "ğŸ¨ DaveCustomNode/Multi-Image"
    
    def process_kontext_editing(self, base_image, kontext_prompt, editing_mode, aspect_ratio, 
                               prompt_upsampling, guidance_scale, enable_iterative_editing, 
                               max_iterations, preserve_character, context_strength,
                               source_image_1=None, source_image_2=None, source_image_3=None, 
                               source_image_4=None, reference_style=None, 
                               image_1_prompt="ç¼–è¾‘å›¾åƒ1åŒºåŸŸï¼Œä¿æŒè‡ªç„¶è¿‡æ¸¡", 
                               image_2_prompt="ç¼–è¾‘å›¾åƒ2åŒºåŸŸï¼Œä¿æŒè‡ªç„¶è¿‡æ¸¡",
                               image_3_prompt="ç¼–è¾‘å›¾åƒ3åŒºåŸŸï¼Œä¿æŒè‡ªç„¶è¿‡æ¸¡", 
                               image_4_prompt="ç¼–è¾‘å›¾åƒ4åŒºåŸŸï¼Œä¿æŒè‡ªç„¶è¿‡æ¸¡",
                               previous_edit=None):
        """
        å¤„ç†Kontextç”Ÿå›¾ç¼–è¾‘ - å¤šå›¾åŒºåŸŸèåˆ
        
        Args:
            base_image: åŸºç¡€å›¾åƒ
            kontext_prompt: Kontextç¼–è¾‘æç¤ºè¯
            editing_mode: ç¼–è¾‘æ¨¡å¼ï¼ˆå±€éƒ¨ç¼–è¾‘/é£æ ¼è½¬æ¢/è§’è‰²ä¸€è‡´æ€§/å¤šè½®ç¼–è¾‘ï¼‰
            aspect_ratio: è¾“å‡ºå®½é«˜æ¯”
            prompt_upsampling: æ˜¯å¦å¯ç”¨æç¤ºè¯å¢å¼º
            guidance_scale: å¼•å¯¼å¼ºåº¦
            enable_iterative_editing: æ˜¯å¦å¯ç”¨è¿­ä»£ç¼–è¾‘
            max_iterations: æœ€å¤§è¿­ä»£æ¬¡æ•°
            preserve_character: æ˜¯å¦ä¿æŒè§’è‰²ä¸€è‡´æ€§
            context_strength: ä¸Šä¸‹æ–‡ç†è§£å¼ºåº¦
            source_image_1-4: æºå›¾åƒ1-4
            reference_style: å‚è€ƒé£æ ¼å›¾åƒ
            image_1-4_prompt: æ¯ä¸ªå›¾åƒå¯¹åº”çš„ç¼–è¾‘æç¤ºè¯
            previous_edit: ä¹‹å‰çš„ç¼–è¾‘ç»“æœï¼ˆç”¨äºè¿­ä»£ï¼‰
            
        Returns:
            edited_image: ç¼–è¾‘åçš„å›¾åƒ
            edit_log: ç¼–è¾‘æ—¥å¿—
            kontext_metadata: Kontextå…ƒæ•°æ®
            combined_mask: ç»„åˆé®ç½©
            stitch_preview: æ‹¼æ¥é¢„è§ˆ
        """
        from datetime import datetime
        
        try:
            # æ—¥å¿—è®°å½•å¼€å§‹
            start_time = datetime.now()
            edit_log = f"ğŸ¨ Kontextç¼–è¾‘å¼€å§‹ - {start_time.strftime('%Y-%m-%d %H:%M:%S')}\n"
            edit_log += f"ğŸ“ ç¼–è¾‘æ¨¡å¼: {editing_mode}\n"
            edit_log += f"ğŸ“ è¾“å‡ºæ¯”ä¾‹: {aspect_ratio}\n"
            edit_log += f"ğŸ¯ æç¤ºè¯: {kontext_prompt[:100]}...\n"
            
            # è·å–è¾“å…¥å›¾åƒä¿¡æ¯
            base_tensor = base_image
            batch_size, height, width, channels = base_tensor.shape
            edit_log += f"ğŸ–¼ï¸ åŸºç¡€å›¾åƒå°ºå¯¸: {width}x{height}\n"
            
            # æ”¶é›†æºå›¾åƒ
            source_images = []
            active_sources = []
            
            for i, source_img in enumerate([source_image_1, source_image_2, source_image_3, source_image_4]):
                if source_img is not None:
                    source_images.append(source_img)
                    active_sources.append(i + 1)
                    edit_log += f"âœ… æºå›¾åƒ{i+1}: å·²åŠ è½½\n"
            
            # å¤„ç†æ–‡æœ¬æç¤ºè¯
            image_prompts = [image_1_prompt, image_2_prompt, image_3_prompt, image_4_prompt]
            active_prompts = []
            for i, prompt in enumerate(image_prompts):
                if prompt and prompt.strip():
                    active_prompts.append(prompt)
                    edit_log += f"ğŸ“ å›¾åƒ{i+1}æç¤ºè¯: {prompt[:50]}...\n"
            
            # æ ¹æ®ç¼–è¾‘æ¨¡å¼å¤„ç†
            if editing_mode == "local_edit":
                # å±€éƒ¨ç¼–è¾‘æ¨¡å¼ - ç²¾ç¡®åŒºåŸŸæ§åˆ¶
                result_image = self._process_local_edit(
                    base_tensor, source_images, active_prompts, kontext_prompt, 
                    guidance_scale, context_strength
                )
                edit_log += "ğŸ¯ å±€éƒ¨ç¼–è¾‘æ¨¡å¼: ç²¾ç¡®åŒºåŸŸæ§åˆ¶å®Œæˆ\n"
                
            elif editing_mode == "style_transfer":
                # é£æ ¼è½¬æ¢æ¨¡å¼
                result_image = self._process_style_transfer(
                    base_tensor, reference_style, kontext_prompt, 
                    preserve_character, context_strength
                )
                edit_log += "ğŸ¨ é£æ ¼è½¬æ¢æ¨¡å¼: ä¿æŒå†…å®¹ç»“æ„å®Œæˆ\n"
                
            elif editing_mode == "character_consistency":
                # è§’è‰²ä¸€è‡´æ€§æ¨¡å¼
                result_image = self._process_character_consistency(
                    base_tensor, source_images, kontext_prompt, 
                    context_strength
                )
                edit_log += "ğŸ‘¤ è§’è‰²ä¸€è‡´æ€§æ¨¡å¼: ç‰¹å¾ä¿æŒå®Œæˆ\n"
                
            elif editing_mode == "multi_round_edit":
                # å¤šè½®ç¼–è¾‘æ¨¡å¼
                result_image = self._process_multi_round_edit(
                    base_tensor, previous_edit, kontext_prompt, 
                    max_iterations, guidance_scale
                )
                edit_log += f"ğŸ”„ å¤šè½®ç¼–è¾‘æ¨¡å¼: æœ€å¤§{max_iterations}è½®è¿­ä»£\n"
            
            # åˆ›å»ºç©ºçš„ç»„åˆé®ç½©ï¼ˆå·²ç§»é™¤é®ç½©åŠŸèƒ½ï¼‰
            combined_mask = torch.zeros((1, height, width), dtype=torch.float32)
            
            # åˆ›å»ºæ‹¼æ¥é¢„è§ˆ
            stitch_preview = self._create_stitch_preview(source_images, base_tensor)
            
            # ç”ŸæˆKontextå…ƒæ•°æ®
            kontext_metadata = self._generate_kontext_metadata(
                editing_mode, aspect_ratio, guidance_scale, 
                len(source_images), preserve_character, context_strength
            )
            
            # åº”ç”¨å®½é«˜æ¯”è°ƒæ•´
            if aspect_ratio != "match_input":
                result_image = self._apply_aspect_ratio(result_image, aspect_ratio)
                edit_log += f"ğŸ“ å®½é«˜æ¯”è°ƒæ•´: {aspect_ratio}\n"
            
            # å®Œæˆæ—¥å¿—
            end_time = datetime.now()
            processing_time = (end_time - start_time).total_seconds()
            edit_log += f"â±ï¸ å¤„ç†å®Œæˆï¼Œè€—æ—¶: {processing_time:.2f}ç§’\n"
            edit_log += f"âœ¨ Kontextç¼–è¾‘æˆåŠŸ - æ”¯æŒåŒºåŸŸç”Ÿå›¾\n"
            
            return (result_image, edit_log, kontext_metadata, combined_mask, stitch_preview)
            
        except Exception as e:
            error_msg = f"âŒ Kontextç¼–è¾‘é”™è¯¯: {str(e)}\nè¯¦ç»†ä¿¡æ¯: {repr(e)}"
            print(error_msg)
            
            # è¿”å›åŸå›¾å’Œé”™è¯¯ä¿¡æ¯
            empty_mask = torch.zeros((1, height, width), dtype=torch.float32)
            return (base_image, error_msg, "ERROR", empty_mask, base_image)
    
    def _process_local_edit(self, base_image, source_images, prompts, main_prompt, guidance_scale, context_strength):
        """å¤„ç†å±€éƒ¨ç¼–è¾‘ - Kontextæ ¸å¿ƒåŠŸèƒ½ï¼Œä½¿ç”¨æ–‡æœ¬æç¤ºè¯æ§åˆ¶"""
        # è¿™é‡Œå®ç°å±€éƒ¨ç¼–è¾‘çš„æ ¸å¿ƒé€»è¾‘
        # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™é‡Œä¼šè°ƒç”¨Kontext APIæˆ–æœ¬åœ°æ¨¡å‹
        result = base_image.clone()
        
        # æ¨¡æ‹ŸåŸºäºæ–‡æœ¬æç¤ºè¯çš„å±€éƒ¨ç¼–è¾‘æ•ˆæœ
        for i, (source_img, prompt) in enumerate(zip(source_images, prompts)):
            if source_img is not None and prompt and prompt.strip():
                # åŸºäºæ–‡æœ¬æç¤ºè¯çš„åŒºåŸŸæ··åˆ
                # è¿™é‡Œå¯ä»¥æ ¹æ®promptçš„å†…å®¹è°ƒæ•´æ··åˆå¼ºåº¦
                prompt_strength = context_strength
                if "å¼ºçƒˆ" in prompt or "æ˜æ˜¾" in prompt:
                    prompt_strength = min(1.0, context_strength * 1.5)
                elif "è½»å¾®" in prompt or "æ·¡åŒ–" in prompt:
                    prompt_strength = context_strength * 0.5
                    
                # ç®€å•çš„åŒºåŸŸæ··åˆï¼ˆå®é™…åº”ç”¨ä¸­ä¼šä½¿ç”¨æ›´å¤æ‚çš„AIå¤„ç†ï¼‰
                result = result * (1 - prompt_strength) + source_img * prompt_strength
        
        return result
    
    def _process_style_transfer(self, base_image, reference_style, prompt, preserve_character, context_strength):
        """å¤„ç†é£æ ¼è½¬æ¢"""
        result = base_image.clone()
        
        if reference_style is not None:
            # æ¨¡æ‹Ÿé£æ ¼è½¬æ¢
            style_factor = 0.3 if preserve_character else 0.7
            result = result * (1 - style_factor) + reference_style * style_factor
        
        return result
    
    def _process_character_consistency(self, base_image, source_images, prompt, context_strength):
        """å¤„ç†è§’è‰²ä¸€è‡´æ€§"""
        result = base_image.clone()
        
        # æ¨¡æ‹Ÿè§’è‰²ä¸€è‡´æ€§ä¿æŒ
        if source_images:
            for source_img in source_images:
                result = result * 0.8 + source_img * 0.2 * context_strength
        
        return result
    
    def _process_multi_round_edit(self, base_image, previous_edit, prompt, max_iterations, guidance_scale):
        """å¤„ç†å¤šè½®ç¼–è¾‘"""
        if previous_edit is not None:
            # åŸºäºä¹‹å‰ç¼–è¾‘ç»“æœç»§ç»­
            result = previous_edit.clone()
        else:
            result = base_image.clone()
        
        # æ¨¡æ‹Ÿè¿­ä»£ç¼–è¾‘
        for i in range(min(max_iterations, 3)):  # é™åˆ¶æ¨¡æ‹Ÿè¿­ä»£æ¬¡æ•°
            result = result * 0.95 + base_image * 0.05
        
        return result
    
    def _create_combined_mask(self, masks, width, height):
        """åˆ›å»ºç»„åˆé®ç½©"""
        if not masks:
            return torch.zeros((1, height, width), dtype=torch.float32)
        
        combined = torch.zeros((1, height, width), dtype=torch.float32)
        for mask in masks:
            if mask.shape[1:] != (height, width):
                # è°ƒæ•´é®ç½©å°ºå¯¸
                mask = torch.nn.functional.interpolate(
                    mask.unsqueeze(0), size=(height, width), mode='bilinear'
                ).squeeze(0)
            combined = torch.maximum(combined, mask)
        
        return combined
    
    def _create_stitch_preview(self, source_images, base_image):
        """åˆ›å»ºæ‹¼æ¥é¢„è§ˆ"""
        if not source_images:
            return base_image
        
        # ç®€å•çš„æ°´å¹³æ‹¼æ¥é¢„è§ˆ
        all_images = [base_image] + source_images
        
        # è°ƒæ•´æ‰€æœ‰å›¾åƒåˆ°ç›¸åŒé«˜åº¦
        target_height = base_image.shape[1]
        resized_images = []
        
        for img in all_images:
            if img.shape[1] != target_height:
                img = torch.nn.functional.interpolate(
                    img.permute(0, 3, 1, 2), 
                    size=(target_height, img.shape[2]), 
                    mode='bilinear'
                ).permute(0, 2, 3, 1)
            resized_images.append(img)
        
        # æ°´å¹³æ‹¼æ¥
        stitched = torch.cat(resized_images, dim=2)
        return stitched
    
    def _generate_kontext_metadata(self, editing_mode, aspect_ratio, guidance_scale, 
                                  num_sources, preserve_character, context_strength):
        """ç”ŸæˆKontextå…ƒæ•°æ®"""
        metadata = {
            "editing_mode": editing_mode,
            "aspect_ratio": aspect_ratio,
            "guidance_scale": guidance_scale,
            "num_source_images": num_sources,
            "preserve_character": preserve_character,
            "context_strength": context_strength,
            "kontext_version": "1.0",
            "supports_iterative": True,
            "supports_regional": True
        }
        
        return str(metadata)
    
    def _apply_aspect_ratio(self, image, aspect_ratio):
        """åº”ç”¨å®½é«˜æ¯”è°ƒæ•´"""
        batch_size, height, width, channels = image.shape
        
        # å®šä¹‰å®½é«˜æ¯”æ˜ å°„
        ratios = {
            "1:1": (1, 1),
            "2:3": (2, 3),
            "3:2": (3, 2),
            "9:16": (9, 16),
            "16:9": (16, 9),
            "4:3": (4, 3),
            "3:4": (3, 4)
        }
        
        if aspect_ratio in ratios:
            w_ratio, h_ratio = ratios[aspect_ratio]
            
            # è®¡ç®—æ–°å°ºå¯¸
            if width / height > w_ratio / h_ratio:
                new_height = height
                new_width = int(height * w_ratio / h_ratio)
            else:
                new_width = width
                new_height = int(width * h_ratio / w_ratio)
            
            # è°ƒæ•´å›¾åƒå¤§å°
            resized = torch.nn.functional.interpolate(
                image.permute(0, 3, 1, 2),
                size=(new_height, new_width),
                mode='bilinear'
            ).permute(0, 2, 3, 1)
            
            return resized
        
        return image

# èŠ‚ç‚¹æ˜ å°„
NODE_CLASS_MAPPINGS = {
    "MultiImageAreaEditor": MultiImageAreaEditor,
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "MultiImageAreaEditor": "ğŸ¨ å¤šå›¾åŒºåŸŸç¼–è¾‘å™¨ (æ–‡æœ¬æ§åˆ¶)",
} 